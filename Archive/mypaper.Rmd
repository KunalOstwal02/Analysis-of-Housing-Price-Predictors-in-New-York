---
title: House Prices in New York City

# Use letters for affiliations
author:
  - name: Ishaan Syed Zahiruddin | Jonah Smith | Kunal Ostwal | Luke Gallagher | Yibo Zhao

Optional: Group CC08E2
# Could be used, for example, to mention the bibliographic info in a post-print.
# If not specified, defaults to "This version was compiled on \today"
#date_subtitle: Published in *Journal of Statistical Software*, 2018

# For footer text  TODO(fold into template, allow free form two-authors)
#lead_author_surname: Author and Author

# Place eg a DOI URL or CRAN Package URL here
doi_footer: "CC08E2"

# Abstract
abstract: |
    The goal was to find the possible price prediction for each of the houses. To find features of the dataset that may have a strong correlation with Price, a heat map of the correlation between each pair of variables was generated. The heatmap showed us that Bathrooms, Rooms and Land.Value have a Strong correlation coefficient with Price. Living.Area had the greatest correlation coefficient with Price while Age has a small relationship with Price. With the help of a simple linear model between Living.Area and Price and a multi-variate model where we used both the Forward Stepwise Function(FSF) and Backward Stepwise Function(BSF) which produced the same model. In sample performance showed us that multivariate model has a better goodness of fit compared to the univariate model. 10-fold cross validation test confirmed our findings. From the top three variables with the strongest ability to predict housing price, Price and Living.Area had the highest correlation relationship. This meant that the size of the property increased the price.

# Optional: Acknowledgements
acknowledgements: |


# Optional: One or more keywords
keywords:
  - Regression models
  - Stepwise AIC
  - NYC Real Estate

# Paper size for the document, values of letter and a4
papersize: letter

# Font size of the document, values of 9pt (default), 10pt, 11pt and 12pt
fontsize: 9pt

header-includes:
  - \usepackage{leading}
  - \leading{0pt}
# Optional: Force one-column layout, default is two-column
#one_column: true

# Optional: Enables lineno mode, but only if one_column mode is also true
#lineno: true

# Optional: Enable one-sided layout, default is two-sided
#one_sided: true

# Optional: Enable section numbering, default is unnumbered
#numbersections: true

# Optional: Specify the depth of section number, default is 5
#secnumdepth: 5

# Optional: Skip inserting final break between acknowledgements, default is false
skip_final_break: true

# Optional: Bibliography 
bibliography: pinp

# Optional: Enable a 'Draft' watermark on the document
#watermark: true

# Customize footer, eg by referencing the vignette
footer_contents: "New York Housing Data"

# Produce a pinp document
output: pinp::pinp

# Required: Vignette metadata for inclusion in a package.
vignette: >
  %\VignetteIndexEntry{YourPackage-vignetteentry}
  %\VignetteKeywords{YourPackage, r, anotherkeyword}
  %\VignettePackage{YourPackage}
  %\VignetteEngine{knitr::rmarkdown}
---
```{css, echo = FALSE}
line-height:3
```
## Introduction 

The USA was hit hard by the COVID pandemic and New York City in particular was the hardest hit city worldwide. The goal of our project was to find the best possible price prediction for each of the houses. The dependent variable was Price of the house. A preliminiary look at the dataset showed us that the New York real estate market is a buyer's market with total sales to listing ratio of 0.12 i.e. the supply of homes is much higher than the demand for homes. Hence, it is a buyer's market. 


## Dataset Description
The dataset used by us is a random sample of houses from a much larger dataset called the Saratoga Housing Data. The dataset has both categorical and numerical variables which will help to make a more comprehensive prediction. We take it for granted that the data has been independently collected. Since it is a random sample of a larger dataset, the data is assumed to be void of any comfounding factors related to sampling and therefore, we assume there is independence between error terms. Data Cleaning was not an ardous process as the dataset used by us was cleaned by the authors. There were no empty cells. There was an extra variable Test which is not a part of the original Saratoga dataset. Since we do not have a description of it, we omit it from our analysis to remove any comfounding factors from the result.

```{r echo=FALSE, message=FALSE}
library(dplyr) # v. 1.0.7
library(ggplot2) # v. 3.3.5
library(purrr) # v. 0.3.4
library(broom) # v. 0.7.9
library(ggplot2)
library(knitr)
library(car)
library(gridExtra)
library(tidyverse)
library(scales)

houses <- read.table(file = "./resources/housing-prices-ge19.txt", sep = "\t", header = TRUE)

houses <- houses %>% 
  mutate(
    Waterfront = factor(Waterfront),
    New.Construct = factor(New.Construct),
    Central.Air = factor(Central.Air),
    Fuel.Type = factor(Fuel.Type),
    Heat.Type = factor(Heat.Type),
    Sewer.Type = factor(Sewer.Type),
    Test = factor(Test)
  )

```

## Analysis  


### Simple Linear Model 
There are a number features of the data set such as `Living.Area`, `Age`, `Land.Value`, that seem like they ought to prima facie have a strong correlation with `Price`. Indeed, a heat map of the correlation between each pair of variables shows a number of interesting things.
 
```{r, echo=FALSE, message=FALSE, fig.align='center', fig.height=2, fig.width=3}
# select numeric or integer type factors
temp = select(houses, - which(sapply(houses, class) == "factor"))
cor_mat = cor(temp)
melted_cor_mat = cor_mat %>%
  data.frame() %>% 
  rownames_to_column(var = "var1") %>% 
  gather(key = "var2", value = "cor", -var1)
# correlation matrix heatmap
melted_cor_mat %>% ggplot() + 
  aes(x=var1, y=var2, fill=cor) + 
  geom_tile() + theme_minimal(base_size = 7) +
  scale_fill_gradient2(
    low = "red", high = "darkgreen", mid = "white", 
    midpoint = 0, limit = c(-1,1)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 0.1, size = 5), 
        axis.text.y = element_text(hjust = 0.1, size = 5))
```

The features `Bathrroms`, `Rooms`, and `Land.Value` all have a strong correlation coefficient with `Price`;
- surprisingly, `Age` has a small relationship with `Price`;
- less suprisingly, `Living.Area` has the greatest correlation coefficient with `Price` of $0.71$.

#### Price vs Living Area
```{r, echo=FALSE, message=FALSE, fig.width = 3, fig.height = 2, fig.align='center'}
slm = lm(Price ~ (Living.Area), data = houses)

houses %>% ggplot() + 
  aes(y = Price, x = (Land.Value)) + 
  geom_point(alpha = 0.25) + 
  geom_smooth(method = "lm") + 
  labs(x = "Living Area (square feet)", y = "Price (US dollars)") + 
  scale_y_continuous(labels = comma)
```
 - Consequently, we decided to produce a simple linear model between `Living.Area` and `Price`, described by the equation
$$\text{Price} = `r format(slm$coefficients[1], scientific=FALSE)` + `r format(slm$coefficients[2], scientific=FALSE)` * \text{Living.Area}$$.  
\vspace{-8mm}

### Stepwise Models  
```{r echo=FALSE, message=FALSE, include=FALSE}
M0 = lm(Price ~ 1, data = houses)
Mf = lm(Price ~ . - Test, data = houses)
step_forward = step(M0, scope = list(lower = M0, upper = Mf), 
                    direction = "forward", trace = FALSE)
step_back = step(Mf, direction = "backward", trace = FALSE)
```
After looking at the result of the simple linear model, we generated a stepwise model to test the dataset's performance in a complex model. We tested both models: the forward and backward AIC function and ended up with the same model. 
\begin{equation}
  \begin{aligned}
Price&=`r round(step_forward$coefficients[1], digits = 2)`\ +\ `r round(step_forward$coefficients[2], digits = 2)`\ * \text{Living.Area}\ +\ `r round(step_forward$coefficients[3], digits = 2)`\ * \text{Land.Value} \\
&+\ `r round(step_forward$coefficients[4], digits = 2)` * \text{Bathrooms} + `r round(step_forward$coefficients[5], digits = 2)` * \text{Waterfront}\   \\
& `r round(step_forward$coefficients[6], digits = 2)` * \text{New.Construct} + `r round(step_forward$coefficients[7], digits = 2)` * \text{Heat.Type(Hot Air)}\ \\
& `r round(step_forward$coefficients[8], digits = 2)` * \text{Heat.Type(Hot Water)}\ `r round(step_forward$coefficients[9], digits = 2)` * \text{Heat.Type(None)}\\
&+\ `r round(step_forward$coefficients[10], digits = 2)` * \text{Lot.Size} - `r round(step_forward$coefficients[11], digits = 2)` * \text{Central.Air}\\
& `r round(step_forward$coefficients[12], digits = 2)` * \text{Age} + `r round(step_forward$coefficients[13], digits = 2)` * \text{Rooms}  `r round(step_forward$coefficients[14], digits = 2)` * \text{Bedrooms}\\
       \label{eqn:example}
  \end{aligned}
\end{equation}  

From the generated equation it can be seen that the following factors were not added to the model by the FSF and were excluded by the BSF: `Pct.College`, `Fireplaces`, `Fireplaces`, `Sewer.Type`, and `Fuel.Type`.  

## Performance Analysis

### In-sample Performance

Here we compare the performance of the multivariate regression model to that of the simple linear regression model within the dataset.
\begin{equation}
  \begin{aligned}
&\text{Simple Model}\ r^2: `r summary(slm)$r.squared`\\
&\text{Multivariate Model}\ r^2: `r summary(step_forward)$r.squared`
       \label{eqn:example}
  \end{aligned}
\end{equation}  
Hence, the multivariate model has a better goodness of fit compared to the univariate model.

### Out of Sample Performance  

We chose Mean Absolute Error over Root Mean Square Error since there are several outliers in the data.
```{r echo=FALSE}
set.seed(2)
n = nrow(houses)
```
```{r echo=FALSE, }
n_train = floor(0.8*n)
n_test = n - n_train
grp_labs = rep(c("Train", "Test"), times = c(n_train, n_test))
houses$grp = sample(grp_labs)
train_dat = houses %>% filter(grp == "Train")
lm_simple_train = lm(Price ~ Living.Area, data = train_dat)
lm_full_train = lm(formula = Price ~ Lot.Size + Waterfront + Age + 
    Land.Value + New.Construct + Central.Air + Heat.Type + Living.Area + 
    Bedrooms + Bathrooms + Rooms, data = train_dat)
test_dat = houses %>% filter(grp == "Test")
simple_pred = predict(lm_simple_train, newdata = test_dat)
full_pred = predict(lm_full_train, newdata = test_dat)
```

We compared the mean absolute error of the two models.  
 \begin{equation}
  \begin{aligned}
&\text{MAE(Univariate Model):}\ `r format(round(mean(abs(test_dat$Price - simple_pred)), 2), scientific = FALSE)`\\  
&\text{MAE(Multivariate Model):}\ `r format(round(mean(abs(test_dat$Price - full_pred)), 2), scientific = FALSE)`  
       \label{eqn:example}
  \end{aligned}
\end{equation}  
The multivariate model has the lower mean absolute error, as such it is better at predicting the prices of New York houses than the linear model.

### 10-fold Cross Validation

We performed a 10-fold cross validation test on the two models. 

```{r echo=FALSE, message =FALSE}
set.seed(2)
```

```{r echo=FALSE, message =FALSE}
houses$grp = NULL
fold_id = c(rep(1:17, each = 102))
houses$fold_id = sample(fold_id, replace = FALSE)
```

```{r echo=FALSE, message=FALSE}
k = 10
simple_mse = full_mse = vector(mode = "numeric", length = k)
simple_mae = full_mae = vector(mode = "numeric", length = k)
for (i in 1:k) {
  test_set = houses[fold_id == i,]
  training_set = houses[fold_id != i,]
  simple_lm = lm(Price ~ Living.Area, data = training_set)
  simple_pred = predict(simple_lm, test_set)
  simple_mse[i] = mean((test_set$Price - simple_pred) ^ 2)
  simple_mae[i] = mean(abs(test_set$Price - simple_pred))
  full_lm = lm(formula = Price ~ Lot.Size + Waterfront + Age + 
    Land.Value + New.Construct + Central.Air + Heat.Type + Living.Area + 
    Bedrooms + Bathrooms + Rooms, data = training_set)
  full_pred = predict(full_lm, test_set)
  full_mse[i] = mean((test_set$Price - full_pred)^2)
  full_mae[i] = mean(abs(test_set$Price - full_pred))
}
```

```{r echo=FALSE }
cv_res = tibble(simple_mse, full_mse, simple_mae, full_mae)
```
```{r echo=FALSE, message=TRUE, fig.height=2, fig.width=3}
cv_res %>% gather(key = "metric", value = "error") %>% 
  separate(col = metric, into = c("model","metric")) %>% 
  ggplot(aes(x = model, y = error)) + facet_wrap(~metric, scales = "free_y") + 
  geom_boxplot() 
```

## Assumptions  
#### For Univariate model  
Linearity: The residuals plotted in the univariate model appear symmetrically distributed above and below zero with some outliers above zero, therefore the data is assumed to be linear.
```{r echo=FALSE, fig.height=2, fig.align='center'}
# Checking assumtpions - Linearity and Homoskedacity
lm = lm(Price ~ Living.Area, data = houses)

plot(lm$residuals, xlab = "Index", ylab = "Residuals", main = "Univariate Linear Model") 
abline(h=0, lwd = 3, col = "red")
```
Homoskedacity: In the model,it appears the variance is constant in the residuals plot for each error term and therefore the homoskedacity assumption is satisfied.

#### For Multivariate model:
### Linearity
The residuals plotted in the multivariate model appear symmetrically distributed above and below zero with some outliers above zero, therefore the data is assumed to be linear.  
```{r echo=FALSE, fig.height=2, fig.align='center'}
# Checking assumtpions - Linearity and Homoskedacity
plot(Mf$residuals, xlab = "Index", ylab = "Residuals", main = "Multivariate Linear Model")	
abline(h=0, lwd = 3, col = "red")
```
### Homoskedacity
In the univariate model, it appears the variance is constant in the residuals plot for each error term and therefore the homoskedacity assumption is satisfied.

### Normality  
The majority of points lie close to the diagonal line in both QQ plots, however, there are many outliers in the upper tail so the normality assumption is moderately well satisfied.
```{r echo=FALSE, fig.align='center', fig.height=1.5}
# Checking assumptions - Normality
lm.res = lm$residuals
Mf.res = Mf$residuals
g1 = ggplot(data.frame(lm.res)) +
  aes(sample = lm.res) + 
  geom_qq_line() + 
  geom_qq(size=1)
g2 = ggplot(data.frame(Mf.res)) +
  aes(sample = Mf.res) + 
  geom_qq_line() + 
  geom_qq(size=1)
grid.arrange(g1, g2, nrow=1, ncol=2)
```
### Independence  
Since it is a random sample of a larger dataset, the data is assumed to be void of any confounding factors related to sampling and therefore, we assume there is independence between error terms. 



## Results  

From our analysis it was found that the top three variables with the strongest ability to predict housing price were Land.Value, Living.Area and Bathrooms. Of these three variables, Price and Living.Area have the highest correlation relationship. When thinking about these results, it seems quite clear that the size of a property would increase the price and the larger the property, the more likely the higher the Land.Value. Bathrooms are an interesting addition, however, it would be fair to say that the larger a house is the more bathrooms it is likely to have. While these seem like obvious predictors of housing prices, these potential assumptions are backed up by the analysis we have undertaken.

In a multiple regression, all factors contributes to the houses price.In answer to our inference: what is set of features that best predicts  `Price` we found that:

\begin{equation}
  \begin{aligned}
Price&=`r round(step_forward$coefficients[1], digits = 2)`\ +\ `r round(step_forward$coefficients[2], digits = 2)`\ * \text{Living.Area}\ +\ `r round(step_forward$coefficients[3], digits = 2)`\ * \text{Land.Value} \\
&+\ `r round(step_forward$coefficients[4], digits = 2)` * \text{Bathrooms} + `r round(step_forward$coefficients[5], digits = 2)` * \text{Waterfront}\   \\
& `r round(step_forward$coefficients[6], digits = 2)` * \text{New.Construct} + `r round(step_forward$coefficients[7], digits = 2)` * \text{Heat.Type(Hot Air)}\ \\
& `r round(step_forward$coefficients[8], digits = 2)` * \text{Heat.Type(Hot Water)}\ `r round(step_forward$coefficients[9], digits = 2)` * \text{Heat.Type(None)}\\
&+\ `r round(step_forward$coefficients[10], digits = 2)` * \text{Lot.Size} - `r round(step_forward$coefficients[11], digits = 2)` * \text{Central.Air}\\
& `r round(step_forward$coefficients[12], digits = 2)` * \text{Age} + `r round(step_forward$coefficients[13], digits = 2)` * \text{Rooms}  `r round(step_forward$coefficients[14], digits = 2)` * \text{Bedrooms}\\
       \label{eqn:example}
  \end{aligned}
\end{equation} 

## Discussion & Conclusions

The increase in housing prices in New York has resulted in many home buyers questioning where and what they can buy, therefore, being able to accurately predict house price based on its attributes is of growing importance. Our analysis led us to the conclusion that there are specific attributes that can help predict housing price, and by using a simple linear model between living.Area and Price we discovered that these two properties had the highest correlation relationship. It would make sense that the size of a home would correlate to an increase in house price, and therefore the need for this prediction may not entirely be necessary. However, in sample performance using a multivariate model, the other variables with the strongest prediction value were highlighted. Bathrooms were found to be a strong predictor of house prices, and showed a trend towards increased price with increased bathroom amount. 

While the models are clear and hold in predicting housing prices in New York City there are still limitations to our research. The data used to create the models has a significant number of outliers which have the potential to sway any clear results that may be obtained. Our research also does not investigate trends of specific subgroups of price (such as Fuel.Type) and therefore there may be trends that give rise to situations such as the Simpsons paradox which may decrease the accuracy of the model prediciton. 

In summary, our analysis found that the multivariable model generated by the FSF is a good predictor of house prices, and improves the accuracy of any univariate linear model, having satisfied the assumptions required of a linear model.


